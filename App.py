import sys
import cv2
import numpy as np
import mediapipe as mp
from keras.models import load_model
import time
from collections import deque
from PyQt5.QtWidgets import QApplication, QMainWindow, QMessageBox
from PyQt5.QtCore import Qt, QTimer
from PyQt5.QtGui import QImage, QPixmap
from UI import Ui_MainWindow

# --- Load emotion model ---
model = load_model('custom_cnn_fer.h5')
labels_dict = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Neutral', 5: 'Sad', 6: 'Surprise'}

# --- Kh·ªüi t·∫°o MediaPipe ---
mp_face_mesh = mp.solutions.face_mesh
mp_face_detection = mp.solutions.face_detection

# G·∫Øn m·ª©c ƒë·ªô hi·ªÉu cho c·∫£m x√∫c
EMOTION_WEIGHTS = {
    'Happy': 1.3,
    'Surprise': 1.2,
    'Neutral': 1.1,
    'Sad': 0.8,
    'Angry': 0.7,
    'Fear': 0.8,
    'Disgust': 0.7,
    'Unknown': 0.6
}

# L·ªõp ch√≠nh k·∫ø th·ª´a t·ª´ QMainWindow v√† file thi·∫øt k·∫ø giao di·ªán UI
class FocusAnalysisApp(QMainWindow, Ui_MainWindow):
    def __init__(self):
        super().__init__()
        self.setupUi(self)   # Thi·∫øt l·∫≠p giao di·ªán t·ª´ file Qt Designer

        # K·∫øt n·ªëi button
        self.start_btn.clicked.connect(self.start_analysis)
        self.stop_btn.clicked.connect(self.stop_analysis)
        self.report_btn.clicked.connect(self.generate_report)

        # Initialize variables
        # Kh·ªüi t·∫°o c√°c bi·∫øn theo d√µi tr·∫°ng th√°i v√† d·ªØ li·ªáu ph√¢n t√≠ch
        self.is_analyzing = False  # C·ªù cho bi·∫øt c√≥ ƒëang ph√¢n t√≠ch kh√¥ng
        self.start_time = 0  # Th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu ph√¢n t√≠ch
        self.cap = None  # ƒê·ªëi t∆∞·ª£ng webcam
        self.face_mesh = None  # D√πng ƒë·ªÉ l·∫•y ƒëi·ªÉm landmark khu√¥n m·∫∑t (Face Mesh)
        self.face_detection = None  # D√πng ƒë·ªÉ ph√°t hi·ªán khu√¥n m·∫∑t
        self.focus_levels = []  # Danh s√°ch ƒëi·ªÉm t·∫≠p trung theo th·ªùi gian
        self.timestamps = []  # M·ªëc th·ªùi gian t∆∞∆°ng ·ª©ng v·ªõi m·ªói ƒëi·ªÉm
        self.understanding_score = 50  # ƒêi·ªÉm hi·ªÉu b√†i ban ƒë·∫ßu (trung b√¨nh)
        self.understanding_scores = [50]  # L·ªãch s·ª≠ ƒëi·ªÉm hi·ªÉu b√†i
        self.understanding_timestamps = [0]  # Th·ªùi ƒëi·ªÉm ghi nh·∫≠n ƒëi·ªÉm hi·ªÉu b√†i
        self.last_understanding_update = 0  # L·∫ßn cu·ªëi c·∫≠p nh·∫≠t ƒëi·ªÉm hi·ªÉu b√†i
        self.delay_between_understanding_update = 1.5  # Gi√£n c√°ch t·ªëi thi·ªÉu gi·ªØa c√°c l·∫ßn c·∫≠p nh·∫≠t ƒëi·ªÉm hi·ªÉu b√†i
        self.smoothing_window = 15  # S·ªë l∆∞·ª£ng ƒëi·ªÉm ƒë·ªÉ l√†m m∆∞·ª£t d·ªØ li·ªáu
        self.focus_history = deque(maxlen=self.smoothing_window)  # H√†ng ƒë·ª£i ch·ª©a ƒëi·ªÉm t·∫≠p trung ƒë·ªÉ l√†m m∆∞·ª£t

        # Setup timer
        self.timer = QTimer()
        self.timer.timeout.connect(self.update_frame) # G·ªçi h√†m update_frame m·ªói l·∫ßn timeout

        # Giao di·ªán tr·∫°ng th√°i ban ƒë·∫ßu
        self.status_label.setText("S·∫µn s√†ng b·∫Øt ƒë·∫ßu theo d√µi")
        self.status_label.setStyleSheet("color: #4CAF50; font-weight: bold;")

    #B·∫Øt ƒë·∫ßu ph√¢n t√≠ch
    def start_analysis(self):
        if not self.is_analyzing:
            # Initialize video capture
            self.cap = cv2.VideoCapture(0) # M·ªü webcam (camera 0)
            if not self.cap.isOpened():
                self.show_error("Kh√¥ng th·ªÉ m·ªü camera!")
                return

            # Kh·ªüi t·∫°o m√¥-ƒëun Mediapipe
            self.face_mesh = mp_face_mesh.FaceMesh(refine_landmarks=True, min_detection_confidence=0.5)
            self.face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.5)
            #face_mesh: theo d√µi 468 ƒëi·ªÉm landmark tr√™n khu√¥n m·∫∑t.
            # face_detection: x√°c ƒë·ªãnh khu√¥n m·∫∑t trong khung h√¨nh.
            # ƒê·ªô tin c·∫≠y t·ªëi thi·ªÉu l√† 50%.

            # Reset d·ªØ li·ªáu theo d√µi
            self.start_time = time.time()
            self.focus_levels = []
            self.timestamps = []
            self.understanding_score = 50
            self.understanding_scores = [50]
            self.understanding_timestamps = [0]
            self.last_understanding_update = self.start_time
            self.focus_history = deque(maxlen=self.smoothing_window)

            # Update UI
            self.is_analyzing = True
            self.start_btn.setEnabled(False) #t·∫Øt
            self.stop_btn.setEnabled(True)
            self.status_label.setText("ƒêang theo d√µi...")
            self.status_label.setStyleSheet("color: #2196F3; font-weight: bold;")
            self.timer.start(30)   # G·ªçi update_frame m·ªói 30ms

    def stop_analysis(self):
        if self.is_analyzing:
            self.timer.stop() #d·ª´ng cam
            if self.cap:
                self.cap.release()
                self.cap = None

            self.is_analyzing = False
            self.start_btn.setEnabled(True)
            self.stop_btn.setEnabled(False)
            self.status_label.setText("ƒê√£ d·ª´ng theo d√µi")
            self.status_label.setStyleSheet("color: #F44336; font-weight: bold;") #m√†u ƒë·ªè

            # Hi·ªÉn th·ªã khung h√¨nh ƒëen khi d·ª´ng
            black_image = np.zeros((480, 640, 3), dtype=np.uint8)
            self.display_image(black_image) #hi·ªán khung l√™n giao di·ªán

    #∆∞·ªõc l∆∞·ª£ng g√≥c ƒë·∫ßu
    def estimate_pose(self, landmarks, img_w, img_h):
        indices = [33, 263, 1, 61, 291, 199] #Ch·ªçn 6 ƒëi·ªÉm landmark quan tr·ªçng (m·∫Øt, m≈©i, mi·ªáng) ƒë·ªÉ ƒë·ªãnh v·ªã t∆∞ th·∫ø ƒë·∫ßu
        # Chuy·ªÉn ƒë·ªïi sang t·ªça ƒë·ªô pixel tr√™n ·∫£nh (2D)
        pts_2d = [[landmarks[idx].x * img_w, landmarks[idx].y * img_h] for idx in indices]

    #T·∫≠p c√°c ƒëi·ªÉm 3D gi·∫£ ƒë·ªãnh (gi·ªëng c·∫•u tr√∫c khu√¥n m·∫∑t trung b√¨nh) trong kh√¥ng gian th·ª±c.
        model_points = np.array([
            [-30, 0, 0],
            [30, 0, 0],
            [0, 0, 0],
            [-25, -30, 0],
            [25, -30, 0],
            [0, -65, 0],
        ], dtype=np.float32)

        # Chuy·ªÉn ƒë·ªïi c√°c ƒëi·ªÉm ·∫£nh sang float32 ƒë·ªÉ d√πng v·ªõi solvePnP
        image_points = np.array(pts_2d, dtype=np.float32)
        focal_length = img_w

        # Ma tr·∫≠n n·ªôi t·∫°i camera (camera matrix) gi·∫£ ƒë·ªãnh ƒë∆°n gi·∫£n
        camera_matrix = np.array([[focal_length, 0, img_w / 2],
                                  [0, focal_length, img_h / 2],
                                  [0, 0, 1]], dtype="double")
        # Kh√¥ng d√πng m√©o ·ªëng k√≠nh (gi·∫£ ƒë·ªãnh camera l√Ω t∆∞·ªüng)
        dist_coeffs = np.zeros((4, 1))

        # D·ª± ƒëo√°n g√≥c quay (rotation vector) d√πng solvePnP
        success, rotation_vector, _ = cv2.solvePnP(model_points, image_points, camera_matrix, dist_coeffs)

        # Chuy·ªÉn vector quay sang ma tr·∫≠n quay (rotation matrix)
        rmat, _ = cv2.Rodrigues(rotation_vector)
        # T√≠nh to√°n g√≥c pitch, yaw, roll t·ª´ rotation matrix
        angles, _, _, _, _, _ = cv2.RQDecomp3x3(rmat)
        return angles  # pitch, yaw, roll

    #ƒêi·ªÉm t·∫≠p trung c·ªßa c√°c h∆∞·ªõng ƒë·∫ßu
    def head_orientation_factor(self, yaw):
        abs_yaw = abs(yaw)
        if abs_yaw <= 40:
            return 1.0
        elif abs_yaw >= 80:
            return 0.6
        else:
            return 0.8

    def emotion_score(self, emotion_label, confidence):
        if emotion_label == "Unknown":
            return 0.0, -2

        weight = EMOTION_WEIGHTS.get(emotion_label, 1.0)
        focus_multiplier = confidence * weight
    #t√≠nh ƒëi·ªÉm d·ª±a v√†o c·∫£m x√∫c
        if emotion_label in ['Happy', 'Surprise']:
            return focus_multiplier, 1
        elif emotion_label == 'Neutral':
            return focus_multiplier, 0
        else:
            return focus_multiplier, -1 #c√°c c·∫£m x√∫c ti√™u c·ª±c

    #Trung b√¨nh h√≥a ƒëi·ªÉm t·∫≠p trung trong c·ª≠a s·ªï tr∆∞·ª£t
    def calculate_smoothed_focus(self):
        if not self.focus_history:
            return 0
        return sum(self.focus_history) / len(self.focus_history)

    def update_frame(self): #update frame cho t·ª´ng khung ·∫£nh
        if not self.cap or not self.cap.isOpened():
            return

        ret, frame = self.cap.read() # ƒê·ªçc 1 khung h√¨nh t·ª´ webcam
        if not ret:
            return

        h, w, _ = frame.shape
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #chuy·ªÉn ·∫£nh sang rgb

        # ===== Ph√¢n ph√°t hi·ªán khu√¥n mat =========

        #bi·∫øn kh·ªüi t·∫°o
        emotion_label = "Unknown"
        confidence = 0.0
        face_roi = None

        #d√πng mediapipe face detection ƒë·ªÉ t√¨m ·∫£nh khu√¥n m·∫∑t x√°m
        detection_results = self.face_detection.process(rgb)
        if detection_results.detections:
            # Ch·ªçn khu√¥n m·∫∑t l·ªõn nh·∫•t
            largest_face = max(detection_results.detections,
                               key=lambda det: det.location_data.relative_bounding_box.width *
                                               det.location_data.relative_bounding_box.height)

            # L·∫•y t·ªça ƒë·ªô bounding box t∆∞∆°ng ƒë·ªëi r·ªìi chuy·ªÉn sang t·ªça ƒë·ªô tuy·ªát ƒë·ªëi (pixel)
            bbox = largest_face.location_data.relative_bounding_box
            x = int(bbox.xmin * w)
            y = int(bbox.ymin * h)
            width = int(bbox.width * w)
            height = int(bbox.height * h)
            # ƒê·∫£m b·∫£o kh√¥ng v∆∞·ª£t qu√° k√≠ch th∆∞·ªõc ·∫£nh
            x, y, width, height = max(0, x), max(0, y), min(w, width), min(h, height)
            face_roi = frame[y:y + height, x:x + width]

            if face_roi.size > 0:
                #=========Ph·∫ßn nh·∫≠n di·ªán c·∫£m x√∫c===========
                gray_roi = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)
                resized = cv2.resize(gray_roi, (48, 48))
                normalize = resized / 255.0 # Chu·∫©n h√≥a gi√° tr·ªã pixel v·ªÅ [0, 1]
                reshaped = np.reshape(normalize, (1, 48, 48, 1))
                result_predict = model.predict(reshaped, verbose=0)

                label = np.argmax(result_predict) # D·ª± ƒëo√°n c·∫£m x√∫c
                confidence = np.max(result_predict)
                emotion_label = labels_dict[label]
                #L·∫•y nh√£n c·∫£m x√∫c d·ª± ƒëo√°n v√† ƒë·ªô tin c·∫≠y cao nh·∫•t

        # Head pose estimation
        focus = 0
        mesh_results = self.face_mesh.process(rgb)  #ch·ª©a k·∫øt qu·∫£ nh·∫≠n di·ªán mesh (l∆∞·ªõi ƒëi·ªÉm) khu√¥n m·∫∑t t·ª´ ·∫£nh RGB
        if mesh_results.multi_face_landmarks and face_roi is not None:
            landmarks = mesh_results.multi_face_landmarks[0].landmark
            pitch, yaw, roll = self.estimate_pose(landmarks, w, h) #Tr√≠ch xu·∫•t landmark khu√¥n m·∫∑t ƒë·∫ßu ti√™n ƒë·ªÉ ∆∞·ªõc l∆∞·ª£ng g√≥c xoay ƒë·∫ßu

            # T√≠nh ƒëi·ªÉm t·∫≠p trung
            focus_multiplier, understanding_delta = self.emotion_score(emotion_label, confidence)
            orientation_factor = self.head_orientation_factor(yaw) #bi·∫øn
            focus = 100 * focus_multiplier * orientation_factor #ƒêi·ªÉm t·∫≠p trung (focus) = c·∫£m x√∫c √ó h∆∞·ªõng ƒë·∫ßu.
            focus = max(0, min(100, focus))

        # ============C·∫≠p nh·∫≠t m·ª©c ƒë·ªô t·∫≠p trung=========
        self.focus_history.append(focus)  #L∆∞u l·∫°i ƒëi·ªÉm t·∫≠p trung hi·ªán t·∫°i.
        smoothed_focus = self.calculate_smoothed_focus() #D√πng trung b√¨nh tr∆∞·ª£t ƒë·ªÉ l√†m m∆∞·ª£t m·ª©c t·∫≠p trung
        #Ghi l·∫°i l·ªãch s·ª≠ ƒëi·ªÉm t·∫≠p trung v√† m·ªëc th·ªùi gian.
        self.focus_levels.append(smoothed_focus)
        self.timestamps.append(time.time() - self.start_time)

        # ==========C·∫≠p nh·∫≠t m·ª©c ƒë·ªô hi·ªÉu b√†i===========
        current_time = time.time()
        #C·∫≠p nh·∫≠t hi·ªÉu b√†i n·∫øu ƒë·ªß th·ªùi gian tr√¥i qua t·ª´ l·∫ßn c·∫≠p nh·∫≠t tr∆∞·ªõc
        if current_time - self.last_understanding_update > self.delay_between_understanding_update:
            if smoothed_focus < 50:
                self.understanding_score -= 1 #N·∫øu t·∫≠p trung th·∫•p, gi·∫£m ƒëi·ªÉm hi·ªÉu b√†i.
            else:
                _, base_delta = self.emotion_score(emotion_label, confidence) #N·∫øu t·∫≠p trung >= 50, t√≠nh m·ª©c tƒÉng ƒëi·ªÉm hi·ªÉu b√†i t·ª´ c·∫£m x√∫c

                #N·∫øu t·∫≠p trung cao, tƒÉng ƒëi·ªÉm hi·ªÉu b√†i nhi·ªÅu h∆°n.
                if smoothed_focus >= 70:
                    self.understanding_score += base_delta + 1
                elif smoothed_focus >= 85:
                    self.understanding_score += base_delta + 2
                else:
                    self.understanding_score += base_delta

            self.understanding_score = max(0, min(100, self.understanding_score))
            self.understanding_scores.append(self.understanding_score)
            self.understanding_timestamps.append(current_time - self.start_time)
            self.last_understanding_update = current_time

        # =======C·∫≠p nh·∫≠t l·∫°i UI========
        self.update_ui(frame, emotion_label, confidence, smoothed_focus, current_time)

    def update_ui(self, frame, emotion_label, confidence, smoothed_focus, current_time):
        # Hi·ªán th·ªã h√¨nh ·∫£nh webcam
        self.display_image(frame)

        # C·∫≠p nh·∫≠t ƒë·ªìng h·ªì ƒëo m·ª©c t·∫≠p trung v√† hi·ªÉu b√†i.
        self.focus_gauge.setValue(smoothed_focus)
        self.understanding_gauge.setValue(self.understanding_score)

        # C·∫≠p nh·∫≠t nh√£n c·∫£m x√∫c v√† ƒë·ªô tin c·∫≠y.
        self.emotion_label.setText(emotion_label)
        self.confidence_label.setText(f"ƒê·ªô tin c·∫≠y: {confidence * 100:.1f}%")

        #Hi·ªÉn th·ªã th·ªùi gian ƒë√£ h·ªçc
        elapsed = current_time - self.start_time
        mins = int(elapsed // 60)
        secs = int(elapsed % 60)
        self.time_label.setText(f"Th·ªùi gian: {mins:02d}:{secs:02d}")

        # C·∫£nh b√°o n·∫øu ng∆∞·ªùi h·ªçc kh√¥ng t·∫≠p trung.
        if smoothed_focus < 50:
            self.warning_label.setText("C·∫¢NH B√ÅO: M·ª©c ƒë·ªô t·∫≠p trung th·∫•p!")
        else:
            self.warning_label.setText("")

        # C·∫≠p nh·∫≠t m√†u
        color = "#2196F3"  # Default blue
        if emotion_label == "Happy":
            color = "#FFC107"  # Yellow
        elif emotion_label == "Angry":
            color = "#F44336"  # Red
        elif emotion_label == "Sad":
            color = "#2196F3"  # Blue
        elif emotion_label == "Surprise":
            color = "#9C27B0"  # Purple

        self.emotion_label.setStyleSheet(f"color: {color};")

        # C·∫≠p nh·∫≠t tr·∫°ng th√°i
        if smoothed_focus >= 70:
            self.status_label.setText("T·∫≠p trung t·ªët")
            self.status_label.setStyleSheet("color: #4CAF50; font-weight: bold;")
        elif smoothed_focus >= 50:
            self.status_label.setText("T·∫≠p trung trung b√¨nh")
            self.status_label.setStyleSheet("color: #FFC107; font-weight: bold;")
        else:
            self.status_label.setText("T·∫≠p trung th·∫•p")
            self.status_label.setStyleSheet("color: #F44336; font-weight: bold;")

    #Chuy·ªÉn ·∫£nh t·ª´ OpenCV sang ƒë·ªãnh d·∫°ng QPixmap v√† hi·ªÉn th·ªã l√™n giao di·ªán PyQt.
    def display_image(self, frame):
        # Chuy·ªÉn sang RGB
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        h, w, ch = rgb_frame.shape
        bytes_per_line = ch * w

        # Create QImage
        qt_image = QImage(rgb_frame.data, w, h, bytes_per_line, QImage.Format_RGB888)
        pixmap = QPixmap.fromImage(qt_image)

        # Scale and display
        self.camera_label.setPixmap(pixmap.scaled(
            self.camera_label.width(),
            self.camera_label.height(),
            Qt.KeepAspectRatio
        ))
    #=====T·∫°o b√°o c√°o=======
    def generate_report(self):
        if not self.focus_levels:
            self.show_error("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ xu·∫•t b√°o c√°o!")
            return

        # Calculate statistics
        total_time = self.timestamps[-1] if self.timestamps else 1
        avg_focus = np.mean(self.focus_levels) if self.focus_levels else 0
        min_focus = np.min(self.focus_levels) if self.focus_levels else 0
        max_focus = np.max(self.focus_levels) if self.focus_levels else 0

        #T√≠nh th·ªùi gian h·ªçc, ƒë·ªô t·∫≠p trung trung b√¨nh, min/max
        low_focus_time = sum(1 for f in self.focus_levels if f < 50) * (total_time / len(self.focus_levels))
        medium_focus_time = sum(1 for f in self.focus_levels if 50 <= f < 80) * (total_time / len(self.focus_levels))
        high_focus_time = sum(1 for f in self.focus_levels if f >= 80) * (total_time / len(self.focus_levels))

        # Th·ªùi gian ph√¢n b·ªë theo c√°c m·ª©c ƒë·ªô t·∫≠p trung.
        low_percent = (low_focus_time / total_time) * 100
        medium_percent = (medium_focus_time / total_time) * 100
        high_percent = (high_focus_time / total_time) * 100

        # Chuy·ªÉn sang ph·∫ßn trƒÉm ƒë·ªÉ tr·ª±c quan h√≥a.
        if avg_focus >= 70 and self.understanding_score >= 70:
            performance_rating = "XU·∫§T S·∫ÆC"
            feedback = "B·∫°n ƒë√£ duy tr√¨ s·ª± t·∫≠p trung v√† hi·ªÉu b√†i xu·∫•t s·∫Øc!"
        elif avg_focus >= 60 and self.understanding_score >= 60:
            performance_rating = "T·ªêT"
            feedback = "K·∫øt qu·∫£ t·ªët, h√£y ti·∫øp t·ª•c ph√°t huy!"
        elif avg_focus >= 50 and self.understanding_score >= 50:
            performance_rating = "TRUNG B√åNH"
            feedback = "K·∫øt qu·∫£ trung b√¨nh, c·∫ßn c·∫£i thi·ªán s·ª± t·∫≠p trung"
        else:
            performance_rating = "C·∫¶N C·∫¢I THI·ªÜN"
            feedback = "H√£y xem l·∫°i ph∆∞∆°ng ph√°p h·ªçc t·∫≠p ƒë·ªÉ c·∫£i thi·ªán k·∫øt qu·∫£"

        # Create report text
        report_text = (
            f"===== B√ÅO C√ÅO BU·ªîI H·ªåC =====\n\n"
            f"‚è± Th·ªùi l∆∞·ª£ng: {int(total_time // 60)} ph√∫t {int(total_time % 60)} gi√¢y\n"
            f"üéØ T·∫≠p trung trung b√¨nh: {avg_focus:.1f}%\n"
            f"üìä Hi·ªÉu b√†i cu·ªëi c√πng: {self.understanding_score}%\n\n"
            f"üìà PH√ÇN B·ªê M·ª®C T·∫¨P TRUNG:\n"
            f"  ‚Ä¢ Th·∫•p (<50%): {low_percent:.1f}%\n"
            f"  ‚Ä¢ Trung b√¨nh (50-80%): {medium_percent:.1f}%\n"
            f"  ‚Ä¢ Cao (>80%): {high_percent:.1f}%\n\n"
            f"üèÜ ƒê√ÅNH GI√Å HI·ªÜU SU·∫§T: {performance_rating}\n"
            f"üí¨ Nh·∫≠n x√©t: {feedback}\n\n"
            f"============================="
        )

        # Show report in a message box
        msg = QMessageBox()
        msg.setWindowTitle("B√°o c√°o bu·ªïi h·ªçc")
        msg.setText(report_text)
        msg.setIcon(QMessageBox.Information)
        msg.exec_()

    def show_error(self, message):
        QMessageBox.critical(self, "L·ªói", message)

    def closeEvent(self, event):
        self.stop_analysis()
        event.accept()

    #===== Ch·∫°y ·ª©ng d·ª•ng=========
if __name__ == "__main__":
    app = QApplication(sys.argv)
    window = FocusAnalysisApp()
    window.show()
    sys.exit(app.exec_())